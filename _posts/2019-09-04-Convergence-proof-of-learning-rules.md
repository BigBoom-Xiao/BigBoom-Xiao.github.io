# 感知机学习规则收敛性证明

感知机的学习规则十分简单，但他非常有效。实际上可以证明：只要权值的解存在，该规则总能收敛到实现期望分类的权值上。

由单神经元感知机的输出可得下式：

$$ a = hardlim(1^{W^T} p + b) $$

网络提供了正确反映网络行为的下述实例：

{p1, t1}, {p2, t2},...,{pq, tq}

其中每个目标的输出 tq 取值 0 或 1

## 记号

为了方便证明过程，引入几个记号：

权值偏置组合为一个向量：

$$ X = \begin{bmatrix}
     1^W\\
     b
\end{bmatrix}$$

同样，在输入向量中也增加一个参数 1 ，以表示偏置输入：

$$ Z_q = \begin{bmatrix}
     P_q\\
     1
\end{bmatrix} $$

现在可以将神经元的净输入值表示为：

$$ n = 1^{W^T} p + b = X^T Z $$

那么感知机学习规则可写为：

$$ X^{new} = X^{old} + eZ $$

误差 e 可以去 1， 0 和 -1。如果 e = 0，那么权值不变；如果 e = 1，则将输入向量和权值向量相加；如果 e = -1，则将权值向量减去输入向量。如果只考虑权值向量发生改变的那些迭代，则该学习规则变为：

$$ X(k) = X(k - 1) + Z'(k - 1) $$

其中Z'(k - 1)是如下集合的元素：

$$ \{ Z_1, Z_2, \dots , Z_Q, -Z_1, -Z_2, \dots , -Z_Q \} $$

现在假设存在对所有 Q 个输入向量进行正确分类的权值向量 $ X^* $，对该权值向量，假设：

$$ 如果 t_q = 1, 那么 X^{*T} Z_q > \delta > 0 $$

以及

$$ 如果 t_q = -1, 那么 X^{*T} Z_q < -\delta < 0 $$

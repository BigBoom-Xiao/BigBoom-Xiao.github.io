# 神经网络学习

关于学习，可定义为：根据与环境的相互作用二发生的行为改变，其结果倒置对外界刺激产生反映的新模式的建立。

学习过程离不开训练，学习过程就是一种训练使得个体在行为上产生较为持久的改变的过程。

神经网络的全体连接权值可以用一个矩阵W表示，其整体反映了神经网络对于所解决问题的只是存储。神经网络能够通过对样本的学习训练，不断改变网络连接权值以及拓扑结构，以使网络的输出无线接近期望的输出。这一过程被称为神经网络的学习或训练，其本质是可变权值的动态调整。

神经网络的学习算法很多，改变权值的规则都可以称之为学习算法。根据一种广泛的分类方法，可归纳为三类：一类是有监督学习，一类是无监督学习，一类是灌输式学习。

## 三类学习规则

### 监督学习

这种学习方式采用的是纠错规则。学习训练过程中需要不断给网络成对的提供一个输入模式和一个期望网络正确输出的模式，称为“标签”（标签信号、教师信号等）。将神经网络的实际输出同期望输出进行比较，当网络输出与期望的标签不符时，根据差错的方向和大小按照一定的规则调整权值，以使下一步网络输出更接近期望。

### 无监督学习

学习过程中需要不断地给网络提供动态输入信息，网络能根据特有的内部结构和学习规则，在输入信息流中发现热河可能存在的模式和规律，同时能根据网络的功能和输入信息调整权值，这个过程称为网络的自组织，其结果是使网络能对属于同一类的模式进行**自动分类**。这种学习模式中，网络权值的调整不取决于外来的标签，可以认为网络学习评价标准隐含于网络内部或者数据样本之间。

在所解决问题的先验信息很少，甚至没有的时候，这种学习就显得更有实际意义。

### 灌输式学习

指先将网络设计成能记忆特别的例子，以后当给定相关例子的输入信息时，例子便被回忆起来。灌输式学习的权值一旦设计好就不再变动，因此许威西是一次性的，而不是训练过程。

## 网络的训练和神经元的学习

网络的运行一般分为训练和工作两个阶段。训练阶段是为了从训练数据中提取隐含的知识和规律，并储存于网络中供工作中使用。

神经元的学习见[2019-08-28-Learning-rule-of-Prceptor.md](/blog/Learning-rule-of-Prceptor)

## 常见学习规则

### Hebb 学习规则

1949 年心理学家 D.O.Hebb 最早提出了关于神经网络学习机理的“突触修正”的假设。该假设指出，当神经元的突触前膜电位和后膜电位同时为正时，突触传导增强，当前膜电位和后膜电位正负相反时，突触传导减弱。也就是说，当神经元 i 和神经元 j 同时出于兴奋状态时，两者之间的连接强度应增强。根据该假设定义权值的方法，称为 Hebb 学习规则。

在 Hebb 学习规则中，学习信号简单地等于神经元输出：

$$ r = f(W_j^T X) $$

权值向量调整公式为：

$$ \Delta W_j = \eta f(W_j^TX)X $$

权值向量中，每个分量调整由下式确定：

$$ \Delta w_{ij} = \eta f(W_j^TX) x_i = \eta o_j x_i \quad \quad (i = 0, 1, \dots, n) $$

上式表明，权值调整量和输入输出的乘积成正比。显然，经常出现的输入模式将对权值向量又最大的影响。在这种情况下，Heeb学习规则需要预先设置前置的饱和值，以防止输入和输出正负始终一致时出现权值无约束增长。

此外，要求权值初始化，即在学习开始前（t = 0），先对 $ W_j(0) $ 赋予零附近的小随机数。

Hebb 学习规则是一种**纯前馈、无监督学习**。

### Perceptron 学习规则

感知机学习规则规定，学习信号等于神经元期望输出（标签）与实际输出只差：

$$ r = d_j - o_j $$

式中，$ d_j $ 为期望的输出， $ o_j = f(W_j^T X) $ 。感知机使用了符号函数作为激活函数，表达式为：

$$ f(W_j^T) = sgn(W_j^T) = \left\{
\begin{aligned}
1  (W_j^T \geq 0) \\
-1  (W_j^T < 0)
\end{aligned}
\right. $$

因此，权值调整公式应为：

$$ \Delta W_j = \eta[d_j - sgn(W_j^T X)] X $$

$$ \Delta w_{ij} = \eta [d_j - sgn(W_j^T X)] x_i \quad (i = 0, 1, 2, \dots , n) $$

式中，当实际输出与期望输出相同时，权值不需要调整；在存在误差的情况下，由于 $ d_j $ 和 $ sgn(W_j^T) \in \{-1, 1\} $ ，权值调整公式可简化为：

$$ \Delta W_j = \pm 2 \eta X $$

感知机学习规则只适用于二进制神经元，初始值可取任意值。

感知机学习规则代表一种有监督学习。

### $\delta$ 学习规则

该规则亦称连续感知机学习规则，与上述离散感知机学习规则并行。其学习信号规定为：

$$ r = [d_j - f(W_j^T X)]f'(W_j^T X) \\= (d_j - o_j)f'(net_j) \quad (式：1) $$

上式定义的学习信号称为 $ \delta $ 。式中，$ f'(W_j^T X) $ 是激活函数 $ f(net_j) $ 的导数。显然该学习规则要求激活函数可导，因此只适合用于有监督学习中定义的连续激活函数，如 Sigmoid 函数。

事实上，该学习规则很容易由输出值与期望值的最小平方误差条件推导出来。定义神经元输出与期望输出之间的平方误差为：

$$ E = \frac{1}{2}(d_j - o_j)^2 = \frac{1}{2}[d_j - f(W_j^T X)]^2 \quad (式：2) $$

其中，误差 E 是权向量 $ W_j $ 的函数。欲使误差 E 最小，$ W_j $ 应与误差的负梯度成正比，即：

$$ \Delta W_j = -\eta \nabla E \quad (式：3) $$

式中，比例系数 $ \eta $ 是一个正常数。由误差公式(式：2)计算，误差梯度为：

$$ \nabla E = -(d_j - o_j)f'(W_j^T X) X $$

由此带入 (式：3) ，可得权值调整计算式：

$$ \Delta W_j = \eta(d_j - o_j)f'(net_j)X $$

可以看出，上式中 $ \eta $ 与 X 之间的部分正是 (式：1) 中定义的学习信号 $ \delta $ 。$ \Delta W_j $ 中每个分量的调整由下式计算：

$$ \Delta w_{ij} = \eta(d_j - o_j)f'(net_j)x_i \quad (i = 0, 1, \dots, n) $$

该学习规则可推广到多层前馈网络中，权值可以初始化任意值。

### LMS 学习规则

1962 年由 Bernard Widrow 和 Marcian Hoff 提出，因为他能使神经元实际输出与期望输出之间的平方差最小，所以称为最小均方规则（LMS）。其学习信号为：

$$ r = d_j - W_j^T X $$

权值调整向量为：

$$ \Delta W_j = \eta(d_j - W_j^T X) X $$

$ \Delta W_j $ 的各个分量为：

$$ \Delta w_{ij} = \eta(d_j - W_j^T X) x_i \quad (i = 0, 1, \dots, n) $$

实际上，LMS规则可以看成是 $ \delta $ 学习规则的一个特殊情况。该学习规则与神经元采用的激活函数无关，因此不需要对激活函数求导，不仅学习速度较快，而且具有较高的精度。权值可初始化为任意值。

### Correlation 学习规则

### Winner-Take-All 学习规则

### Outstar 学习规则
